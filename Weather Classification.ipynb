{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waqasii/WeatherClassificationSystem/blob/main/Weather%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K418I-XPDSpx"
      },
      "outputs": [],
      "source": [
        "## Install data split library\n",
        "\n",
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnBfYRrSDCUk"
      },
      "outputs": [],
      "source": [
        "## Import required packages\n",
        "\n",
        "import json\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import PIL\n",
        "import PIL.Image as PI\n",
        "import splitfolders\n",
        "import shutil\n",
        "# setting seed and clearing session\n",
        "tf.keras.backend.clear_session()# clear session to save model space\n",
        "# setting seed to keep reproducibililty\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ry3qCW2LXrl"
      },
      "outputs": [],
      "source": [
        "# Download and unzip data\n",
        "!wget \"https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/4drtyfjtfy-1.zip\";\n",
        "!unzip \"4drtyfjtfy-1.zip\";\n",
        "!unzip \"dataset2.zip\"\n",
        "\n",
        "#Check all image sizes and store in list\n",
        "l=[]\n",
        "for img in os.listdir('dataset2'):\n",
        "    image=PI.open(\"dataset2/\"+img)\n",
        "    l.append(image.size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqa4ocR4iqoz"
      },
      "outputs": [],
      "source": [
        "## Make folders for all the available classes\n",
        "class_names=[\"cloudy\",\"rain\",\"shine\",\"sunrise\"]\n",
        "for val in class_names:\n",
        "    os.makedirs(\"data/{}\".format(val),exist_ok=True)\n",
        "\n",
        "## Copy the images to their respective folder\n",
        "\n",
        "for img in os.listdir('dataset2'):\n",
        "    for label in class_names:\n",
        "        if label in img:\n",
        "            shutil.copy('./dataset2/'+img,'./data/{}'.format(label))        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWG-KkR6II-F"
      },
      "source": [
        "#### Defining Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrjlgYT2kdHB"
      },
      "outputs": [],
      "source": [
        "img_height = 128\n",
        "img_width = 128\n",
        "dataset_size=len(os.listdir('dataset2'))\n",
        "train_size=.7\n",
        "validation_size=.2\n",
        "test_size=.1\n",
        "split_seed=1  #to ensure same split for data always"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL3ky75huaWX"
      },
      "outputs": [],
      "source": [
        "## Class distribution\n",
        "data_count={val:len(os.listdir('/content/data/{}'.format(val))) for val in os.listdir('data')}\n",
        "print(data_count,'\\n')\n",
        "\n",
        "plt.bar(list(data_count.keys()), data_count.values(),)\n",
        "plt.title(\"Data Distribution of Image\",)\n",
        "plt.ylabel(\"No. of images\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qw7O46KzNm-"
      },
      "outputs": [],
      "source": [
        "## Split Data in train, Validation and test data\n",
        "\n",
        "splitfolders.ratio(\"data\", output=\"output\",\n",
        "    seed=split_seed, ratio=(train_size,validation_size,test_size), group_prefix=None, move=False,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSh2kBDCzvsU"
      },
      "outputs": [],
      "source": [
        "## Classwise distrubution of splitted dataset\n",
        "\n",
        "for val in os.listdir('output'):\n",
        "    for typ in os.listdir('output/'+val):\n",
        "        print(val,\"->\",typ,\"->\",len(os.listdir('/content/output/{}/{}'.format(val,typ))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb0i2E_CstlB"
      },
      "outputs": [],
      "source": [
        "## To save split data\n",
        "# !zip -r -v  'split_data.zip' data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FglVTXLcWS0M"
      },
      "outputs": [],
      "source": [
        "## Convert image and apply augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.5, # Shifting image width by 40%\n",
        "      height_shift_range=0.2,# Shifting image height by 40%\n",
        "      shear_range=0.2,       # Rotation across X-axis by 20%\n",
        "      zoom_range=0.4,        # Image zooming by 30%\n",
        "      horizontal_flip=True,\n",
        "      \n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    \"/content/output/train\",\n",
        "    target_size = (128, 128),\n",
        "    shuffle=True,\n",
        "    seed=1,\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = 8)\n",
        "\n",
        "\n",
        "datagen_test = ImageDataGenerator(rescale=1./255,\n",
        "                                  \n",
        "    )\n",
        "\n",
        "\n",
        "validation_generator=datagen_test.flow_from_directory(\n",
        "    \"/content/output/val\",\n",
        "    target_size = (128, 128),\n",
        "    class_mode = 'categorical',\n",
        "    seed=1,\n",
        "    shuffle=True,\n",
        "    batch_size = 8)\n",
        "\n",
        "test_generator = datagen_test.flow_from_directory(\n",
        "        \"/content/output/test\",\n",
        "         target_size = (128, 128),\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        seed=1,\n",
        "        class_mode='categorical',)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GBFKi6PmyW3"
      },
      "source": [
        "##LABELS dict for datagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFjAXo1D3Ph5"
      },
      "outputs": [],
      "source": [
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v , k) for k , v in labels.items())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSrna7URWu_q"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "# optimizer: 'adam','rmsprop'\n",
        "# monitor: what parameter to monitor for earlystopping ( loss,val_loss,accuracy)\n",
        "# mode: min or max for monitor\n",
        "# epochs: total no of epochs to try\n",
        "# batch_size: batch size\n",
        "# wait: how long to wait before no change in monitored parameter \n",
        "# c: counter to given numbered name to saved files\n",
        "#########################################\n",
        "\n",
        "\n",
        "\n",
        "def train_model(optimizer,monitor,mode,epochs,batch_size,c,wait=20,activation='relu',metrics = ['accuracy']):\n",
        "    ## Defining model architecture\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        \n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation=activation, input_shape=(128, 128, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation=activation),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Dropout(0.4,seed=1),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation=activation),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Dropout(0.5,seed=1),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation=activation),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Dropout(0.4,seed=1),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        \n",
        "        tf.keras.layers.Dense(128, activation=activation),\n",
        "        \n",
        "        tf.keras.layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss = 'categorical_crossentropy',\n",
        "                optimizer =optimizer ,\n",
        "                metrics = metrics,)\n",
        "    filepath=\"model_weight{}.hdf5\".format(c)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.2,\n",
        "                              patience=5, min_lr=0.00001,)\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor=monitor, verbose=1, save_best_only=True, mode=mode,)\n",
        "    es = EarlyStopping(monitor=monitor,mode=mode,patience=wait,restore_best_weights=True)\n",
        "    callbacks_list = [checkpoint]\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        batch_size=batch_size,  \n",
        "        epochs = epochs,\n",
        "        callbacks=[callbacks_list,reduce_lr],\n",
        "        verbose = 2,\n",
        "        validation_data=validation_generator)\n",
        "    \n",
        "    ##SAVE MODEL ARCHITECTURE AND WEIGHT\n",
        "    \n",
        "    with open('history{}.json'.format(c), 'w') as f:\n",
        "        json.dump(history.history, f)\n",
        "    model.save(\"model{}.h5\".format(c))  \n",
        "    print(\"Model Trained and Saved\")  \n",
        "    os.makedirs('/content/drive/MyDrive/ML Project 1/{}'.format(c),exist_ok=True) \n",
        "    shutil.copy(\"model{}.h5\".format(c) ,\"/content/drive/MyDrive/ML Project 1/{}\".format(c))\n",
        "    shutil.copy(\"model_weight{}.hdf5\".format(c), '/content/drive/MyDrive/ML Project 1/{}'.format(c))\n",
        "    shutil.copy('history{}.json'.format(c), '/content/drive/MyDrive/ML Project 1/{}'.format(c))\n",
        "    \n",
        "\n",
        "    ##SAVE PLOTS OF ACCURACY AND LOSS\n",
        "    fig =plt.figure(figsize=(20, 20))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "    plt.show()\n",
        "    fig.savefig('/content/drive/MyDrive/ML Project 1/{}/plot.jpg'.format(c))\n",
        "\n",
        "    score = model.evaluate(test_generator,verbose=1,)\n",
        "\n",
        "    print('\\n', 'Test accuracy:', score[1]*100,\"%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All models performance "
      ],
      "metadata": {
        "id": "w7KYXMqJ7ByU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "from keras.models import load_model\n",
        "import os \n",
        "x='/content/drive/MyDrive/ML Project 1/'\n",
        "d={}\n",
        "for val in os.listdir(x):\n",
        "    path_to_model = \"{}/model{}.h5\".format((x+val),val)\n",
        "    model = load_model(path_to_model)\n",
        "    Y_pred_res = model.predict_generator(test_generator, test_generator.n // (batch_size+1),)\n",
        "    y_pred_res = np.argmax(Y_pred_res, axis=1)\n",
        "    # y_pred_res\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay,classification_report\n",
        "    class_names = ['Cloudy', 'Rain', 'Shine', 'Sunrise']\n",
        "    n = 14\n",
        "\n",
        "    image_batch, classes_batch = next(test_generator)\n",
        "\n",
        "    for batch in range(n):\n",
        "        temp = next(test_generator)\n",
        "        image_batch = np.concatenate((image_batch, temp[0]))\n",
        "        classes_batch = np.concatenate((classes_batch, temp[1]))\n",
        "\n",
        "    classes_batch = classes_batch\n",
        "    y_predict = model.predict(image_batch)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true = [np.argmax(x) for x in classes_batch],\n",
        "        #y_true = classes_batch.\n",
        "        y_pred = [np.argmax(x) for x in y_predict],\n",
        "        #y_pred = y_predict,\n",
        "        display_labels=class_names,\n",
        "        cmap='Blues'\n",
        "    )\n",
        "    y_true = [np.argmax(x) for x in classes_batch]\n",
        "        #y_true = classes_batch.\n",
        "    y_pred = [np.argmax(x) for x in y_predict]\n",
        "    clf=classification_report( y_true,y_pred,target_names=class_names)\n",
        "    # plt.savefig('./confusion_matrix.png')                                       \n",
        "    plt.show()\n",
        "    print(val)\n",
        "    print(\"\\n\\n\")\n",
        "    print(clf)\n",
        "    score = model.evaluate(test_generator,verbose=1,)\n",
        "    print('\\n', 'Test accuracy:', score[1]*100,\"%\")\n",
        "    d[val]=score[1]*100\n",
        "    print(\"\\n\\n **************\"*10)\n"
      ],
      "metadata": {
        "id": "6Cic-QiPiL7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All plots"
      ],
      "metadata": {
        "id": "qoGBOINd7GLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "x='/content/drive/MyDrive/ML Project 1/'\n",
        "import json\n",
        "import pandas as pd\n",
        "# f=pd.read_json('/content/drive/MyDrive/ML Project 1/1/history1.json',).reset_index(drop=True)\n",
        "for val in os.listdir(x):\n",
        "    # path_to_model = \"{}/model{}.h5\".format((x+val),val)\n",
        "    try:\n",
        "        print(\"***\"*14,val,\"\\n\\n\")\n",
        "            \n",
        "        f=pd.read_json(x+val+\"/history{}.json\".format(val))\n",
        "        ##SAVE PLOTS OF ACCURACY AND LOSS\n",
        "        fig =plt.figure(figsize=(8, 6))\n",
        "        plt.subplot(2, 2, 1)\n",
        "        acc = history['accuracy']\n",
        "        val_acc = history['val_accuracy']\n",
        "        loss = history['loss']\n",
        "        val_loss = history['val_loss']\n",
        "\n",
        "        plt.plot(history['accuracy'])\n",
        "        plt.plot(history['val_accuracy'])\n",
        "        plt.title('model accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(history['loss'])\n",
        "        plt.plot(history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "        plt.show()\n",
        "        \n",
        "    except:\n",
        "        print(\"NOT Parsed\",\"XXX\"*27, val)    \n",
        "        "
      ],
      "metadata": {
        "id": "n2Gq6FDc7JYZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence interval Mc Nemar Test"
      ],
      "metadata": {
        "id": "xSljsmoZ7JAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "                            Classifier2 Correct,\tClassifier2 Incorrect\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    Classifier1 Correct \tYes/Yes\t\t\t\t\tYes/No \n",
        "    Classifier1 Incorrect \tNo/Yes \t\t\t\t\tNo/No"
      ],
      "metadata": {
        "id": "3a-V2MN4tPSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "table = [[106, 5],\n",
        "\t\t [3, 1]]\n",
        "\n",
        "result = mcnemar(table, correction=True,)\n",
        "\n",
        "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
        "\n",
        "alpha = 0.05\n",
        "if result.pvalue > alpha:\n",
        "\tprint('Same proportions of errors (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different proportions of errors (reject H0)')"
      ],
      "metadata": {
        "id": "ckB3HU6GtGh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e31TCB2VDFom"
      },
      "outputs": [],
      "source": [
        "## Some example runs\n",
        "\n",
        "# tf.keras.backend.clear_session()\n",
        "# train_model(adam2,'loss','min',500,32,12,15)\n",
        "# tf.keras.backend.clear_session()\n",
        "# train_model(adam2,'loss','min',500,32,13,15)\n",
        "# tf.keras.backend.clear_session()\n",
        "# train_model(adam3,'loss','min',500,32,14,15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMTF6GNQI1Iy"
      },
      "source": [
        "## Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55zsW5oJhS6"
      },
      "outputs": [],
      "source": [
        "model2 = keras.models.load_model('/content/drive/MyDrive/ML Project 1/good.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1vRw13JI0aX"
      },
      "outputs": [],
      "source": [
        "def test_images(model, num_images = None):  \n",
        "    image_batch, classes_batch = next(test_generator)\n",
        "    predicted_batch = model.predict(image_batch)\n",
        "    for k in range(0,image_batch.shape[0] if num_images is None else num_images):\n",
        "        image = image_batch[k]\n",
        "        real_class = class_names[np.argmax(classes_batch[k],axis=-1)]\n",
        "        predicted_class = class_names[np.argmax(predicted_batch[k],axis=-1)]\n",
        "        value_predicted = predicted_batch[k]\n",
        "        isTrue = (real_class == predicted_class)\n",
        "        plt.figure(k,figsize=(6,4),)\n",
        "        plt.title(\"Prediction - \" + str(\"Correct\\n\" if isTrue else \"Wrong\")\n",
        "         +'\\nActual_Class: ' + real_class + '\\nPredicted_class: ' +\n",
        "          predicted_class +\"\\nScore: \"+ str(np.max(value_predicted)*100)+\"%\\n\")\n",
        "        plt.axis('off')\n",
        "        # plt.savefig('./' + real_class + '_' + predicted_class + '_' + str(value_predicted) + '.png')\n",
        "        plt.imshow(image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njmMaKU38ZC-"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "Y_pred_res = model2.predict_generator(test_generator, test_generator.n // (batch_size+1),)\n",
        "y_pred_res = np.argmax(Y_pred_res, axis=1)\n",
        "y_pred_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqT4oQS5A97G"
      },
      "outputs": [],
      "source": [
        "##https://www.kaggle.com/code/lomitofrito/punto4-parcial-emergentes/notebook\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay,classification_report\n",
        "class_names = ['Cloudy', 'Rain', 'Shine', 'Sunrise']\n",
        "n = 14\n",
        "\n",
        "image_batch, classes_batch = next(test_generator)\n",
        "\n",
        "for batch in range(n):\n",
        "    temp = next(test_generator)\n",
        "    image_batch = np.concatenate((image_batch, temp[0]))\n",
        "    classes_batch = np.concatenate((classes_batch, temp[1]))\n",
        "\n",
        "classes_batch = classes_batch\n",
        "y_predict = model2.predict(image_batch)\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true = [np.argmax(x) for x in classes_batch],\n",
        "    #y_true = classes_batch.\n",
        "    y_pred = [np.argmax(x) for x in y_predict],\n",
        "    #y_pred = y_predict,\n",
        "    display_labels=class_names,\n",
        "    cmap='Blues'\n",
        ")\n",
        "y_true = [np.argmax(x) for x in classes_batch]\n",
        "    #y_true = classes_batch.\n",
        "y_pred = [np.argmax(x) for x in y_predict]\n",
        "clf=classification_report( y_true,y_pred,target_names=class_names)\n",
        "# plt.savefig('./confusion_matrix.png')                                       \n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "print(clf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieGMsTDYEWRh"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('\\n', 'Test accuracy:', score[1]*100,\"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKyV-yrbDIz3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1UN7OVmY1RQomStfPTLM8Qm0ha5qudxwF",
      "authorship_tag": "ABX9TyP2GXfb0LbNSo0F7t522iu8",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}